{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in MNIST jupyter notebook example, I find there are some problems with the lmdb dataset.\n",
    "\n",
    "I doubt if it's the problem of my codes (the lmdb data are correct), so I will use MNIST data downloaded from other sites to create lmdb myself and then use the lmdb I created to continue MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = '../data/caffe2_learning/raw_mnist/train-images-idx3-ubyte.gz'\n",
    "train_label = '../data/caffe2_learning/raw_mnist/train-labels-idx1-ubyte.gz'\n",
    "test_data = '../data/caffe2_learning/raw_mnist/t10k-images-idx3-ubyte.gz'\n",
    "test_label = '../data/caffe2_learning/raw_mnist/t10k-labels-idx1-ubyte.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Necesary modules imported\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import lmdb\n",
    "from caffe2.proto import caffe2_pb2\n",
    "from caffe2.python import workspace, model_helper\n",
    "\n",
    "from caffe2.python.helpers import db_input\n",
    "\n",
    "print('Necesary modules imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ugly codes: used to create MNIST data sets list\n",
    "import gzip\n",
    "def unzip_files(name='train'):\n",
    "    if name == 'train':\n",
    "        data = train_data; label = train_label\n",
    "        num_images = 60000\n",
    "    else:\n",
    "        data = test_data; label = test_label\n",
    "        num_images = 10000\n",
    "    image_size = 28\n",
    "    \n",
    "    data_buf = gzip.open(data, 'rb')\n",
    "    label_buf = gzip.open(label, 'rb')\n",
    "    data_buf.read(16) # I don't know what does thes 16 bytes are, but we need to ignore them\n",
    "    label_buf.read(8) # have no idea\n",
    "    data = np.frombuffer(data_buf.read(), dtype=np.uint8).astype(np.float32)\n",
    "    data = np.reshape(data, (num_images, image_size, image_size), 'C')\n",
    "    data = np.expand_dims(data, axis=1) # NCHW 1\n",
    "    label = np.frombuffer(label_buf.read(), dtype=np.uint8)\n",
    "    label = np.reshape(label, (num_images, -1), 'C')\n",
    "    return [data, label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_data is (60000, 1, 28, 28), label is (60000, 1)\n",
      "Shape of test_data is (10000, 1, 28, 28), label is (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data = unzip_files('train')\n",
    "print('Shape of train_data is {}, label is {}'.format(train_data[0].shape, train_data[1].shape))\n",
    "test_data = unzip_files('test')\n",
    "print('Shape of test_data is {}, label is {}'.format(test_data[0].shape, test_data[1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db(input_data, output_file):\n",
    "    print('>>> Writing database ...')\n",
    "    LMDB_MAP_SIZE = 50 * 1024 ** 3 # max size of this db\n",
    "    NUM_ROWS = input_data[0].shape[0]\n",
    "    env = lmdb.open(output_file, map_size = LMDB_MAP_SIZE)\n",
    "    \n",
    "    with env.begin(write=True) as txn:\n",
    "        for j in range(0, NUM_ROWS):\n",
    "            label = input_data[1][j]\n",
    "            img_data = input_data[0][j]\n",
    "            \n",
    "            tensor_protos = caffe2_pb2.TensorProtos()\n",
    "            img_tensor = tensor_protos.protos.add()\n",
    "            img_tensor.dims.extend(img_data.shape)\n",
    "            img_tensor.data_type = 1\n",
    "            \n",
    "            flatten_img = img_data.reshape(np.prod(img_data.shape))\n",
    "            img_tensor.float_data.extend(flatten_img)\n",
    "            \n",
    "            label_tensor = tensor_protos.protos.add()\n",
    "            label_tensor.data_type = 2\n",
    "            label_tensor.int32_data.append(label)\n",
    "            txn.put(\n",
    "                '{}'.format(j).encode('ascii'),\n",
    "                 tensor_protos.SerializeToString()\n",
    "            )\n",
    "            if j % 10000 == 0:\n",
    "                print('{}% data finished'.format(np.round(j/NUM_ROWS * 100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Writing database ...\n",
      "0.0% data finished\n",
      "16.67% data finished\n",
      "33.33% data finished\n",
      "50.0% data finished\n",
      "66.67% data finished\n",
      "83.33% data finished\n",
      ">>> Writing database ...\n",
      "0.0% data finished\n"
     ]
    }
   ],
   "source": [
    "# create dbs \n",
    "create_db(train_data, '../data/caffe2_learning/raw_mnist/train_lmdb')\n",
    "create_db(test_data, '../data/caffe2_learning/raw_mnist/test_lmdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test read db\n",
    "def read_db(input_file):\n",
    "    print('>>>Test reading this database ...')\n",
    "    model = model_helper.ModelHelper(name='lmdbtest')\n",
    "    batch_size = 32\n",
    "    data, label = model.TensorProtosDBInput(\n",
    "        [], ['data', 'label'], batch_size=batch_size,\n",
    "        db=input_file, db_type='lmdb')\n",
    "    \n",
    "    workspace.RunNetOnce(model.param_init_net)\n",
    "    workspace.CreateNet(model.net)\n",
    "    \n",
    "    cnt = 0\n",
    "    for _ in range(1875): # 60000/32=1875\n",
    "        workspace.RunNet(model.net.Proto().name)\n",
    "        \n",
    "        img_datas = workspace.FetchBlob('data')\n",
    "        labels = workspace.FetchBlob('label')\n",
    "        cnt += img_datas.shape[0]\n",
    "    \n",
    "    assert cnt == 60000 # for test data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-384de32a8318>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mread_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/caffe2_learning/raw_mnist/train_lmdb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'read_db' is not defined"
     ]
    }
   ],
   "source": [
    "read_db('../data/caffe2_learning/raw_mnist/train_lmdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some sample codes to read db\n",
    "input_file = '../data/caffe2_learning/raw_mnist/train_lmdb'\n",
    "\n",
    "model = model_helper.ModelHelper(name='lmdbtest')\n",
    "batch_size = 32\n",
    "data, label = model.TensorProtosDBInput(\n",
    "    [], ['data', 'label'], batch_size=batch_size,\n",
    "    db=input_file, db_type='lmdb')\n",
    "\n",
    "workspace.RunNetOnce(model.param_init_net)\n",
    "workspace.CreateNet(model.net)\n",
    "\n",
    "workspace.RunNet(model.net.Proto().name)\n",
    "\n",
    "img_datas = workspace.FetchBlob('data')\n",
    "labels = workspace.FetchBlob('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1, 28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_datas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
